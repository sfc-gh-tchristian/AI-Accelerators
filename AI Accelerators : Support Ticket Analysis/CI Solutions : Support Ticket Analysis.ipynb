{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "jfhoiw6opuaebmie6j6p",
   "authorId": "320484852755",
   "authorName": "TOMGPT",
   "authorEmail": "tom.christian@snowflake.com",
   "sessionId": "e3dfe9d6-5b03-4ed1-bd1d-c33798e837ee",
   "lastEditTime": 1753441338076
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab10f733-91d9-42b6-a5b1-8f21148e7e74",
   "metadata": {
    "name": "intro",
    "collapsed": false
   },
   "source": "# âœ¨ Support Ticket Analysis powered by Snowflake Cortex\n\n## ðŸŽ¯ Purpose\nThis notebook demonstrates how to analyse customer support tickets using Snowflake's Cortex AI capabilities to identify trends, sentiment patterns, and critical issues that require immediate attention.\n\n## ðŸ’¡ Why do this?\n- ðŸ” **Early Issue Detection**: Identify system-wide problems and outages through pattern recognition\n- ðŸ˜Š **Sentiment Analysis**: Track customer satisfaction across multiple dimensions (brand, product, support)\n- âš¡ **Automated Escalation**: Use AI to determine which tickets require immediate escalation\n- ðŸ“ˆ **Trend Analysis**: Visualize ticket volumes and sentiment patterns over time\n- ðŸš€ **Efficient Support**: Quickly find similar cases using semantic search capabilities\n\n## ðŸ› ï¸ Solution Components\nNote: a combination of SQL & Python will be used for the same outcome. Pick your preferred route!\n\n1. ðŸ“Š **Data Visualisation & EDA**\n   - Use in-built streamlit functionality to see:\n   - Weekly ticket volume by priority\n   - Sentiment trends over time\n\n2. ðŸ¤– **AI-Powered Analysis**\n   - Issue summarization using `AI_AGG`\n   - Automated outage detection with `AI_FILTER`\n   - Multi-dimensional sentiment analysis with `AI_SENTIMENT`\n\n3. âš™ï¸ **Automated Pipeline**\n   - CDC for automated ticket processing\n   - Scheduled updates for search indices\n\n4. ðŸŽ¯ **Accurate Retrieval**\n   - Semantic search for case similarity matching\n   - Quantitative analysis through semantic views \n"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "source": "# If you're running this on the warehouse runtime, please ensure the following packages are included (see top right)\n# snowflake-ml-python\n# snowflake\n#... and that's it!\n\nimport streamlit as st\nimport pandas as pd\nimport altair as alt\nfrom datetime import datetime\n\n#notebooks allow for easy context calling - now we have a permissions/role aligned session in flight\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bf86c94a-259d-416d-886c-ab73633de327",
   "metadata": {
    "language": "python",
    "name": "table_loc_v"
   },
   "outputs": [],
   "source": "# If you'd like to swap to your data - start by defining the table location here:\ntable_loc = 'AI_SOL.SUPPORT.RAW_SUPPORT_TICKETS'\n\n## {{}} allows us to reference the table name - even in SQL!",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "data_preview"
   },
   "source": "-- The dummy data we'll be using is Zendesk style data. \n-- We're particularly interested in the TICKET_DESCRIPTION column.\n-- NOTE! You can use CTRL+F to quickly replace references to your column.\nSELECT * FROM {{table_loc}} LIMIT 10;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f67b1fb6-dcd0-418e-acdb-adb9a5a5aeca",
   "metadata": {
    "language": "python",
    "name": "streamlit_eda",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "## Let's perform some basic exploration using Streamlit visuals and altair\n## NOTE! If you've swapped to your data, remember to select the appropriate columns\n\n# First, let's reference our table\ndf = session.table(table_loc).to_pandas() \n\n# Create a form to contain all selection widgets\nwith st.form(\"visualization_options\"):\n    # Get all datetime columns\n    date_columns = df.select_dtypes(include=['datetime64']).columns.tolist()\n    date_field = st.selectbox('Select Date Field', date_columns)\n\n    # Get categorical columns (object type)\n    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n    category_field = st.selectbox('Select Category Field for Trend Analysis', categorical_columns)\n\n    # Submit button\n    submitted = st.form_submit_button(\"Update Visualizations\")\n\nif submitted or True:  # Show default visualization on first load\n    # Convert selected date to datetime if needed\n    df[date_field] = pd.to_datetime(df[date_field])\n\n    # Display date range and record count\n    st.write(\"Date range in data:\", \n             df[date_field].min().strftime('%Y-%m-%d'), \n             \"to\", \n             df[date_field].max().strftime('%Y-%m-%d'))\n    st.write(\"Number of records:\", len(df))\n\n    # Create trend chart by selected category\n    trend_by_category = alt.Chart(df).mark_line(opacity=0.6).encode(\n        x=alt.X(f'{date_field}:T', \n                title='Week Starting',\n                timeUnit='yearweek',  \n                axis=alt.Axis(format='%Y-%m-%d')\n               ),\n        y=alt.Y('count():Q', title='Number of Records'),\n        color=alt.Color(f'{category_field}:N', title=category_field),\n        tooltip=[\n            alt.Tooltip(f'{date_field}:T', title='Week Starting', timeUnit='yearweek', format='%Y-%m-%d'),\n            alt.Tooltip(f'{category_field}:N'),\n            alt.Tooltip('count():Q', title='Count')\n        ]\n    ).properties(\n        title=f'Weekly Trends by {category_field}',\n        height=300\n    )\n\n    # Display chart\n    st.altair_chart(trend_by_category, use_container_width=True)\n\n    # Show distribution of categories\n    distribution = df[category_field].value_counts()\n    st.write(f\"\\nDistribution of {category_field}:\")\n    st.bar_chart(distribution)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e5197510-7e6c-42fc-9417-2ce742511cca",
   "metadata": {
    "name": "AI_AGG",
    "collapsed": false
   },
   "source": "In the dummy dataset, we have considerably more negative events happening around w/c 11th May. Let's use Cortex and AISQL to dig into those values.\n\nWe'll start with `AI_AGG` - this allows us to ask a singular prompt across multiple rows of data."
  },
  {
   "cell_type": "code",
   "id": "d6857d51-f933-4722-9785-a47dac8df1c5",
   "metadata": {
    "language": "python",
    "name": "agg_func_python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Although we loving call it AISQL - \n# you can either use Python or SQL (see next cell!)\n\nfrom snowflake.snowpark.functions import ai_agg, col, date_trunc\n\ndf = session.table(table_loc)\n\n# Filter for the week with the spike in behaviour\n# Then, based on those tickets, summarise the findings\nresult = df.filter(\n    date_trunc('week', col('SUBMIT_DATE')) == '2025-05-12'\n).agg(\n    ai_agg(\n        col('TICKET_DESCRIPTION'),\n        'What are the top reoccuring issues across these support tickets? Highlight particular trends that would cause negative sentiment issues.'\n    )\n)\n\n# Display the result\nst.write(result.collect()[0][0])\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "167e491f-e18d-45bb-af2e-a036c3d21ae4",
   "metadata": {
    "language": "sql",
    "name": "agg_func_sql"
   },
   "outputs": [],
   "source": "-- This behaviour acts the same, pick and choose based on your preferred language!\nSELECT \nAI_AGG(ticket_description, \n'What are the top reoccuring issues across these support tickets? Highlight particular trends that would cause negative sentiment issues.'\n)\nFROM {{table_loc}}\nWHERE DATE_TRUNC('week',SUBMIT_DATE)='2025-05-12';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d689c567-825d-4e1e-80c8-6a89cc114c94",
   "metadata": {
    "name": "AI_FILTER",
    "collapsed": false
   },
   "source": "In our dummy data, widespread outages appear to be the significant cause for the rise in negative sentiment and overall spike in requests.\n\nLet's use `AI_FILTER` to quickly find customer tickets that mention a system outage as a whole.\n\nRemember, we don't always say what we mean! Using `AI_FILTER` allows us to capture the literal phrase \"outage\" but also instances such as \"everything has gone down\" and \"nothing works\".\n\nWe can even use it to apply a degree of business logic - for example \"does this issue require escalation?\". You may want to provide more detailed guidance in a real life scenario."
  },
  {
   "cell_type": "code",
   "id": "72e22b39-72dc-46c0-bd16-aff6e9e26221",
   "metadata": {
    "language": "sql",
    "name": "filter_func"
   },
   "outputs": [],
   "source": "-- We can use AI_FILTER to both narrow down our dataset as well as act as a column boolean.\n-- Unsurprisingly in this case, the vast majority of system outage emails results in a recommendation to escalate (with zero guidance to the LLM)\n\nSELECT \nTICKET_ID,\nAI_FILTER(PROMPT('Does this ticket require escalation? {0}', TICKET_DESCRIPTION)) as escalate,\nTICKET_DESCRIPTION\nFROM {{table_loc}}\nWHERE\nAI_FILTER(PROMPT('Does this ticket mention a system outage? {0}', TICKET_DESCRIPTION))\nAND\nDATE_TRUNC('week',SUBMIT_DATE)='2025-05-12';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aca39ad1-e513-401b-b410-b9fd5878e514",
   "metadata": {
    "name": "AI_SENTIMENT",
    "collapsed": false
   },
   "source": "Many products capture a basic sentiment score - but rarely tell the full picture. \n\n`AI_SENTIMENT` allows for that breadth of view across multiple key factors. As standard, it'll always provide you with the overall view - but equally allow you to drill into the nuance behind that.\n\nFor example - a review may be positive overall, but could express concern towards the support team."
  },
  {
   "cell_type": "code",
   "id": "945b4c14-f484-4699-81df-75e320636140",
   "metadata": {
    "language": "sql",
    "name": "ai_sent_func"
   },
   "outputs": [],
   "source": "SELECT ticket_id,\nAI_SENTIMENT(ticket_description,\n    ['brand', 'product', 'customer support']) as sentiment_json,\n        sentiment_json:categories[0]:sentiment::STRING AS overall_sentiment,\n        sentiment_json:categories[1]:sentiment::STRING as brand_sentiment,\n        sentiment_json:categories[2]:sentiment::STRING as product_sentiment,\n        sentiment_json:categories[3]:sentiment::STRING as customer_support_sentiment,\n        ticket_description\nFROM \n{{table_loc}}\nWHERE\nDATE_TRUNC('week',SUBMIT_DATE)='2025-05-12';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1305c8a6-c14a-4cc3-a000-66297cec9adc",
   "metadata": {
    "name": "AI_PIPELINES",
    "collapsed": false
   },
   "source": "By themselves, these functions are incredibly powerful. \n\nCombined with the rest of the Snowflake ecosystem, they're even more powerful still. Although the example below doesn't have new fields to work with - it provides an example of how you could process only the new change data (new support tickets) that land in the original table. No need for repeat processing!"
  },
  {
   "cell_type": "code",
   "id": "26580f3f-481e-4c3b-884d-f7151bb494ee",
   "metadata": {
    "language": "sql",
    "name": "AI_pipeline",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- First, create the target table\nCREATE OR REPLACE TABLE PRIORITY_CASES (\n  TICKET_ID VARCHAR,\n  TICKET_DESCRIPTION VARCHAR,\n  CASE_CATEGORY VARCHAR,\n  NEXT_ACTION VARCHAR\n);\n\n-- Create a stream on the source table to capture new records that land in the table\nCREATE\nOR REPLACE STREAM SUPPORT_TICKET_STREAM ON TABLE {{ table_loc }};\n\n--\nCREATE OR REPLACE TASK PROCESS_PRIORITY_CASES \nWAREHOUSE = tc_wh \nSCHEDULE = '60 MINUTE' \nAS MERGE INTO PRIORITY_CASES t USING (\n  SELECT\n    TICKET_ID,\n    TICKET_DESCRIPTION,\n    AI_CLASSIFY(\n      TICKET_DESCRIPTION,\n      ['Feature Request', 'Technical', 'Billing', 'Bug Report', 'General Inquiry']\n    ) as case_category,\n    AI_COMPLETE(\n      'claude-3-7-sonnet',\n      PROMPT(\n        'In under 10 words, suggest the next action an agent should take for this support case: {0}',\n        TICKET_DESCRIPTION\n      )\n    ) as next_action\n  FROM\n    SUPPORT_TICKET_STREAM\n  WHERE\n    METADATA$ACTION = 'INSERT'\n    AND AI_FILTER(\n      PROMPT(\n        'Does this ticket require escalation? {0}',\n        TICKET_DESCRIPTION\n      )\n    )\n) s ON t.TICKET_ID = s.TICKET_ID\nWHEN MATCHED THEN\nUPDATE\nSET\n  t.TICKET_DESCRIPTION = s.TICKET_DESCRIPTION,\n  t.CASE_CATEGORY = s.CASE_CATEGORY,\n  t.NEXT_ACTION = s.NEXT_ACTION\n  WHEN NOT MATCHED THEN\nINSERT\n  (\n    TICKET_ID,\n    TICKET_DESCRIPTION,\n    CASE_CATEGORY,\n    NEXT_ACTION\n  )\nVALUES\n  (\n    s.TICKET_ID,\n    s.TICKET_DESCRIPTION,\n    s.CASE_CATEGORY,\n    s.NEXT_ACTION\n  );\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5ce7557-7a94-4c62-bb91-87967617ce57",
   "metadata": {
    "language": "sql",
    "name": "start_task"
   },
   "outputs": [],
   "source": "-- Don't forget to resume the task once you're happy with your pipeline\n--ALTER TASK PROCESS_PRIORITY_CASES RESUME;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "524bae0d-f119-4707-932c-b719179b48d8",
   "metadata": {
    "name": "SEARCH_AND_RETRIEVAL",
    "collapsed": false
   },
   "source": "# AI Powered Case Matching\nCortex Search enables highly accurate search - using base in class retrieval models - to power RAG use cases.\n\nYou can create a search service directly in Snowflake, either via the UI (AI > Studio > Cortex Search) - or via SQL similar to the code below."
  },
  {
   "cell_type": "code",
   "id": "907542c7-6ed3-45a4-94f7-5582dc02576c",
   "metadata": {
    "language": "sql",
    "name": "create_search_serv"
   },
   "outputs": [],
   "source": "CREATE CORTEX SEARCH SERVICE SUPPORT_SEARCH IF NOT EXISTS\n  ON ticket_description\n  ATTRIBUTES submit_date, ticket_ID\n  WAREHOUSE = tc_wh\n  TARGET_LAG = '1 day'\n  EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\n  AS (\n    SELECT\n        ticket_id,\n        submit_date,\n        customer_id,\n        ticket_description\n    FROM {{table_loc}}\n);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d198ceb-8e60-49ba-ba46-927ada687e7a",
   "metadata": {
    "language": "sql",
    "name": "test_search_SQL"
   },
   "outputs": [],
   "source": "-- We can test how the Search Service works either via the UI - or with a simple SQL query\n\nSELECT PARSE_JSON(\n  SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n      'AI_SOL.SUPPORT.SUPPORT_SEARCH',\n      '{\n        \"query\": \"show me a case where the customer is experiencing internet issues\",\n        \"columns\":[\n            \"ticket_description\",\n            \"submit_date\"\n        ],\n        \"limit\":1\n      }'\n  )\n)['results'] as results;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "84b38d32-0ca0-4882-a6de-463c4e740682",
   "metadata": {
    "language": "python",
    "name": "query_search_serv"
   },
   "outputs": [],
   "source": "# Now let's test our search service in a simple RAG style scenario\nfrom snowflake.core import Root\nfrom snowflake.cortex import complete\n\nroot = Root(session)\n\nsearch_prompt = \"show me a case where the customer is experiencing internet issues\"\n\ntranscript_search_service = (root\n  .databases[\"ai_sol\"]\n  .schemas[\"support\"]\n  .cortex_search_services[\"support_search\"]\n)\n\nresp = transcript_search_service.search(\n  query=search_prompt,\n  columns=[\"ticket_id\", \"ticket_description\"],\n  limit=1\n)\n\n\nmodel = 'claude-3-7-sonnet'\n\nllm_call = complete(model,('Give a one line summary and three short key bullet points about this support case. CASE: '+ resp.to_str()))\n\n\nst.write(llm_call) #LLM response\nst.write('---')\nst.write(resp.to_json()) #closest matching response",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ccdbf8d6-831b-4fad-a91e-436d7ad0408b",
   "metadata": {
    "name": "AI_for_BI",
    "collapsed": false
   },
   "source": "# ðŸ“Š AI for Business Intelligence\n\n## ðŸŽ¯ Purpose\nLearn how to leverage Snowflake's AI capabilities for advanced business intelligence through semantic analysis.\n\n## ðŸ”‘ Key Components\n\n- **Analyst Tool**: Enables quantitative analysis through natural language queries\n- **Data Semantics**: Ensures accuracy and consistency in analysis through defined relationships and metrics\n- **Semantic Views**: Can be created through:\n  - UI-based configuration in Snowflake interface\n  - SQL-based definition (demonstrated in next cell)\n\n## ðŸ’¡ Benefits\n- Natural language querying of your data\n- Consistent metric definitions across your organization\n- Enhanced data discoverability and understanding\n- Improved data governance through semantic layer\n"
  },
  {
   "cell_type": "markdown",
   "id": "53df19de-0086-4514-9a41-e654c78d82e4",
   "metadata": {
    "name": "semantic_view_example",
    "collapsed": false
   },
   "source": "If you'd prefer - head to AI > Studio > Cortex Analyst to use the semantic view generator.\n\n![smodel](https://docs.snowflake.com/en/_images/cortex-analyst-semantic-model-overview.png)"
  },
  {
   "cell_type": "code",
   "id": "5b3687be-333e-49e4-bd22-e86d0094bcce",
   "metadata": {
    "language": "sql",
    "name": "create_semantic_view"
   },
   "outputs": [],
   "source": "-- The SQL interface makes integration with other platforms far simpler.\nCREATE OR REPLACE SEMANTIC VIEW support_analysis \n\n-- Firstly, define your table(s)\n-- Include additional tables where necessary\n-- For example, you may want to blend ticket data with customer data \n-- But no need to have an \"all encompassing\" model, the analyst service can handle many at a time\nTABLES (\n  tickets AS AI_SOL.SUPPORT.RAW_SUPPORT_TICKETS \n  PRIMARY KEY (TICKET_ID) \n  WITH SYNONYMS ('support cases', 'customer tickets') \n  COMMENT = 'Main table for support ticket data'\n)\n\n--If you do add more than one table, be sure to define a relationship like so:\n--RELATIONSHIPS(\n-- join_name AS tbl_1 (join_key) REFERENCES tbl_2\n--)\n\nFACTS (\n  tickets.response_time AS FIRST_RESPONSE_TIME_HOURS,\n  tickets.resolution_time AS RESOLUTION_TIME_HOURS\n) \n\nDIMENSIONS (\n  tickets.submit_date AS SUBMIT_DATE COMMENT = 'Date when the ticket was submitted',\n  tickets.customer_tier AS CUSTOMER_TIER WITH SYNONYMS ('customer level', 'tier') COMMENT = 'Customer tier level',\n  tickets.priority AS PRIORITY COMMENT = 'Ticket priority level. Can be one of HIGH, MEDIUM, or LOW',\n  tickets.product_area AS PRODUCT_AREA WITH SYNONYMS ('product category', 'product type') COMMENT = 'Product area related to the ticket',\n  tickets.status AS STATUS COMMENT = 'Current status of the ticket',\n  tickets.sentiment AS SENTIMENT COMMENT = 'Sentiment analysis of the ticket',\n  tickets.classification AS CLASSIFICATION COMMENT = 'Ticket classification category',\n  tickets.channel AS CHANNEL COMMENT = 'Channel through which ticket was submitted'\n) \n\n-- metrics can also be logically defined\nMETRICS (\n  tickets.ticket_count AS COUNT(TICKET_ID) COMMENT = 'Total number of support tickets',\n  tickets.avg_response_time AS AVG(FIRST_RESPONSE_TIME_HOURS) COMMENT = 'Average first response time in hours',\n  tickets.avg_resolution_time AS AVG(RESOLUTION_TIME_HOURS) COMMENT = 'Average resolution time in hours',\n  tickets.negative_sentiment_rate AS AVG(\n    CASE\n      WHEN SENTIMENT ILIKE '%negative%' THEN 1\n      ELSE 0\n    END\n  ) COMMENT = 'Percentage of tickets with negative sentiment'\n) COMMENT = 'Semantic view for support ticket analysis';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f6c4dcbf-b0d9-4a4f-bd58-166be7800790",
   "metadata": {
    "language": "sql",
    "name": "query_semantic_view"
   },
   "outputs": [],
   "source": "-- Semantic views can be used by AI features (Analyst & Intelligence)\n-- or BI tools via SELECT *.. for example\n-- this is how we'd return the number of tickets per status and the avg response time of those tickets.\n\nSELECT * FROM SEMANTIC_VIEW(\n    support_analysis \n    DIMENSIONS tickets.status\n    METRICS tickets.ticket_count, tickets.avg_response_time\n  );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ed2e3420-4bd4-4d18-acff-704ce069343e",
   "metadata": {
    "name": "finish",
    "collapsed": false
   },
   "source": "# Choose Your Own Adventure\n\nYou now have multiple options of where your AI project should live:\n\n1. Continue to test and refine the Semantic View via Analyst Studio. Simply head to AI > Studio > Cortex Analyst, you'll be able to 'speak' to your semantic view, test how it performs based on standard questions, and refine it further.\n\n2. Integrate with a Streamlit application (or similar). Your semantic view is immediately useable by the Cortex Analyst API. You could opt to add SQL generation into an existing application for data discovery.\n\n3. Unleash Agentic Insight. Both the search and semantic services you created can become tools for an AI agent. Let's define one using the Agent UI - you'll be able to use this agent in conjunction with Snowflake Intelligence (PrPr).\n\n![Snowflake Intelligence](https://github.com/sfc-gh-tchristian/AI-Accelerators/blob/main/AI%20Accelerators%20:%20Support%20Ticket%20Analysis/snowintel.png?raw=true)"
  }
 ]
}